{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import string\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tkinter import messagebox\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Crawler Component\n",
    "URL = \"https://pureportal.coventry.ac.uk/en/organisations/coventry-university/persons/\"\n",
    "profile_url = \"https://pureportal.coventry.ac.uk/en/persons/\"\n",
    "\n",
    "def get_maximum_page():\n",
    "    first = requests.get(URL)\n",
    "    soup = BeautifulSoup(first.text, 'html.parser')\n",
    "    final_page = soup.select('#main-content > div > section > nav > ul > li:nth-child(12) > a')[0]['href']\n",
    "    fp = final_page.split('=')[-1]\n",
    "    return int(fp)\n",
    "\n",
    "def check_department(researcher):\n",
    "    l1 = researcher.find('div', class_='rendering_person_short')\n",
    "    for span in l1.find_all('span'):\n",
    "        if span.text == 'School of Computing, Electronics and Maths':\n",
    "            name = researcher.find('h3', class_='title').find('span').text\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "# def create_csv():\n",
    "#     database = pd.DataFrame(columns=['Title', 'Author', 'Published', 'Link'])\n",
    "#     database.to_csv('database.csv')\n",
    "\n",
    "def update_csv(database):\n",
    "    current_data = pd.read_csv(database, index_col=\"Unnamed: 0\")\n",
    "    return current_data\n",
    "\n",
    "def enter_each_researchers_publication(researcher, url, df):\n",
    "    new_url = url + str(researcher).replace(' ', '-').lower() + '/publications/'\n",
    "    page = requests.get(new_url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find(id=\"main-content\")\n",
    "    papers = results.find_all(\"li\", class_=\"list-result-item\")\n",
    "\n",
    "    for paper in papers:\n",
    "        title = paper.find('h3', class_='title').find('span')\n",
    "        author = paper.find('a', class_='link person').find('span')\n",
    "        date = paper.find('span', class_=\"date\")\n",
    "        link = paper.find('h3', class_='title').find('a', href=True)['href']\n",
    "\n",
    "        opening = pd.read_csv('database.csv', index_col=\"Unnamed: 0\")\n",
    "        opening = opening.append({'Title': title.text,\n",
    "                                  'Author': author.text,\n",
    "                                  'Published': date.text,\n",
    "                                  'Link': link}, ignore_index=True)\n",
    "        opening.to_csv('database.csv')\n",
    "\n",
    "def scrape(mx):\n",
    "    df = update_csv('database.csv')\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i > mx:\n",
    "            break\n",
    "\n",
    "        if i > 0:\n",
    "            url = URL + '?page=' + str(i)\n",
    "        else:\n",
    "            url = URL\n",
    "\n",
    "        i = i + 1\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        results = soup.find(id=\"main-content\")\n",
    "        researchers = results.find_all(\"li\", class_=\"grid-result-item\")\n",
    "\n",
    "        for researcher in researchers:\n",
    "            check = researcher.find('div', class_='stacked-trend-widget')\n",
    "            if check:\n",
    "                name = check_department(researcher)\n",
    "                if name is None:\n",
    "                    pass\n",
    "                else:\n",
    "                    enter_each_researchers_publication(name, profile_url, df)\n",
    "\n",
    "# create_csv()\n",
    "mx = get_maximum_page()\n",
    "scrape(mx)\n",
    "\n",
    "# Indexing Component\n",
    "# Indexing Component\n",
    "scraped_db = pd.read_csv('database.csv', index_col=0)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    processed_text = \" \".join(tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "def create_index():\n",
    "    index = {}\n",
    "    for i, row in scraped_db.iterrows():\n",
    "        title = row['Title']\n",
    "        author = row['Author']\n",
    "        processed_title = preprocess_text(title)\n",
    "        processed_author = preprocess_text(author)\n",
    "        \n",
    "        # Update index with title\n",
    "        if processed_title not in index:\n",
    "            index[processed_title] = []\n",
    "        index[processed_title].append(i)\n",
    "        \n",
    "        # Update index with author\n",
    "        if processed_author not in index:\n",
    "            index[processed_author] = []\n",
    "        index[processed_author].append(i)\n",
    "    \n",
    "    with open('index.json', 'w') as f:\n",
    "        json.dump(index, f)\n",
    "\n",
    "create_index()\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "# Load the index from the JSON file\n",
    "\n",
    "\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the index from the JSON file\n",
    "with open('index.json', 'r') as f:\n",
    "    index = json.load(f)\n",
    "\n",
    "def search_publications():\n",
    "    query = entry.get(1.0, \"end\")\n",
    "    processed_query = preprocess_text(query)\n",
    "    \n",
    "    if processed_query in index:\n",
    "        matching_indices = index[processed_query]\n",
    "        matching_publications = scraped_db.loc[matching_indices]\n",
    "        \n",
    "        # True positives: Matching publications retrieved and relevant\n",
    "        true_positives = len(matching_publications)\n",
    "        \n",
    "        # False negatives: Relevant publications not retrieved\n",
    "        false_negatives = len(scraped_db) - true_positives\n",
    "        \n",
    "        # False positives: Non-relevant publications retrieved\n",
    "        false_positives = 0\n",
    "        \n",
    "        # True negatives: Non-relevant publications not retrieved\n",
    "        true_negatives = 0\n",
    "        \n",
    "        # Calculate false positives and true negatives\n",
    "        for i, row in scraped_db.iterrows():\n",
    "            if i not in matching_indices:\n",
    "                false_positives += 1\n",
    "            else:\n",
    "                true_negatives += 1\n",
    "        \n",
    "        # Calculate the predicted and actual labels\n",
    "        predicted_labels = [1] * true_positives + [0] * false_positives\n",
    "        actual_labels = [1] * (true_positives + false_negatives) + [0] * (false_positives + true_negatives)\n",
    "        \n",
    "        # Adjust the length of the lists if they are different\n",
    "        max_length = max(len(predicted_labels), len(actual_labels))\n",
    "        predicted_labels += [0] * (max_length - len(predicted_labels))\n",
    "        actual_labels += [0] * (max_length - len(actual_labels))\n",
    "        \n",
    "        # Calculate the confusion matrix\n",
    "        cm = confusion_matrix(actual_labels, predicted_labels)\n",
    "        \n",
    "        # Display the confusion matrix\n",
    "        messagebox.showinfo(\"Confusion Matrix\", str(cm))\n",
    "        \n",
    "        # Display the matching publications\n",
    "        messagebox.showinfo(\"Search Results\", matching_publications.to_string())\n",
    "    else:\n",
    "        messagebox.showinfo(\"Search Results\", \"No matching publications found.\")\n",
    "\n",
    "# Rest of the code...\n",
    "\n",
    "# Create the main window\n",
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "window = tk.Tk()\n",
    "window.title(\"Publication Search\")\n",
    "window.geometry(\"600x650\")\n",
    "window.configure(bg=\"#111D88\")\n",
    "window.resizable(0,0)\n",
    "from PIL import ImageTk, Image\n",
    "img=(Image.open(\"coventry-university-logo.png\"))\n",
    "imgg=img.resize((200,200))\n",
    "imggg=ImageTk.PhotoImage(imgg)\n",
    "\n",
    "lbl=Label(window,image=imggg,bg=\"#111D88\")\n",
    "lbl.pack(side=TOP)\n",
    "# Create and position the search label\n",
    "label = tk.Label(window, text=\"Enter author name or title name:\",fg=\"white\",font=(\"Arial\", 20,\"italic\"),bg=\"#111D88\")\n",
    "label.pack(pady=10)\n",
    "\n",
    "# Create and position the search entry\n",
    "entry = tk.Text(window,height=10,width=30,font=(\"Arial\", 20))\n",
    "entry.pack()\n",
    "\n",
    "# Create and position the search button\n",
    "button = tk.Button(window, text=\"Search\",font=(\"Arial\", 20),bg=\"#10164B\",fg=\"white\",command=search_publications)\n",
    "button.pack(pady=10)\n",
    "\n",
    "\n",
    "# Start the main event loop\n",
    "window.mainloop()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
